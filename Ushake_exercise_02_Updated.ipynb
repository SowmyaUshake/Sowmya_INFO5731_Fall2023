{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SowmyaUshake/Sowmya_INFO5731_Fall2023/blob/main/Ushake_exercise_02_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ],
      "metadata": {
        "id": "mAzh1U0sE5I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ],
      "metadata": {
        "id": "PpgvZQdRE-HV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-LmNR3kw9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2daf73c4-d55e-40c5-ec35-b325fc55ec29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nWe require the following information to respond to this research question:\\nAge Information: Compile information on the ages of people who belong to the particular population.\\nInformation regarding these people's heights should be gathered.\\nData on Weight: Obtain information on each person's weight.\\nCalculate each person's BMI using the following formula: BMI = (Weight in kg) / (Height in meters)2.\\nBMI Class Data: Based on BMI values, determine each person's BMI classification (e.g., underweight, normal weight, overweight, or obese).\\nNumber of required data points:\\nThe size and diversity of your population will determine the necessary number of data points. You should aim for a sufficiently broad and diverse sample, which may include information on hundreds or thousands of people, in order to draw relevant conclusions.\\n\\nProcedures for Gathering and Storing the Data:\\n\\nData gathering: Collect information from members of the population you have chosen. Surveys, electronic health data, and physical measurements can all be used to do this.\\n\\nData cleaning: Deal with any missing values or outliers by cleaning up the collected data. Make sure the format of all the data is uniform.\\nCalculate each person's BMI using their weight and height information. For the computation, change height from centimetres to metres.\\nDetermine each person's BMI class using their determined BMI values and the pre-established BMI ranges for classifications.\\nIntegrate all of the gathered and calculated data into a single dataset. Make sure the data is linked by a common identifier (such as a unique ID).\\n\\nData Storage: Organise and safely store the combined dataset. You can utilise a relational database or a spreadsheet format, like CSV.\\nBackup your files frequently to prevent loss of information.\\n\\nData analytic: Examine how age affects BMI and BMI categorization in the population using statistical analytic approaches.\\n\\nVisualisation and Reporting: Produce reports and visualisations to effectively communicate your findings.\\n\\nEthical Considerations: Make sure that data privacy and ethical standards are followed, especially when working with data that relates to your health. Protect the privacy of individuals and, if necessary, anonymize data.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "We require the following information to respond to this research question:\n",
        "Age Information: Compile information on the ages of people who belong to the particular population.\n",
        "Information regarding these people's heights should be gathered.\n",
        "Data on Weight: Obtain information on each person's weight.\n",
        "Calculate each person's BMI using the following formula: BMI = (Weight in kg) / (Height in meters)2.\n",
        "BMI Class Data: Based on BMI values, determine each person's BMI classification (e.g., underweight, normal weight, overweight, or obese).\n",
        "Number of required data points:\n",
        "The size and diversity of your population will determine the necessary number of data points. You should aim for a sufficiently broad and diverse sample, which may include information on hundreds or thousands of people, in order to draw relevant conclusions.\n",
        "\n",
        "Procedures for Gathering and Storing the Data:\n",
        "\n",
        "Data gathering: Collect information from members of the population you have chosen. Surveys, electronic health data, and physical measurements can all be used to do this.\n",
        "\n",
        "Data cleaning: Deal with any missing values or outliers by cleaning up the collected data. Make sure the format of all the data is uniform.\n",
        "Calculate each person's BMI using their weight and height information. For the computation, change height from centimetres to metres.\n",
        "Determine each person's BMI class using their determined BMI values and the pre-established BMI ranges for classifications.\n",
        "Integrate all of the gathered and calculated data into a single dataset. Make sure the data is linked by a common identifier (such as a unique ID).\n",
        "\n",
        "Data Storage: Organise and safely store the combined dataset. You can utilise a relational database or a spreadsheet format, like CSV.\n",
        "Backup your files frequently to prevent loss of information.\n",
        "\n",
        "Data analytic: Examine how age affects BMI and BMI categorization in the population using statistical analytic approaches.\n",
        "\n",
        "Visualisation and Reporting: Produce reports and visualisations to effectively communicate your findings.\n",
        "\n",
        "Ethical Considerations: Make sure that data privacy and ethical standards are followed, especially when working with data that relates to your health. Protect the privacy of individuals and, if necessary, anonymize data.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QpWOgjHi9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3acefa-5d10-4661-875b-655c76ca61c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection and saving complete.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize empty lists for data\n",
        "age_data = []\n",
        "height_data = []\n",
        "weight_data = []\n",
        "\n",
        "# Generate random data for 1000 individuals\n",
        "for _ in range(1000):\n",
        "    age = random.randint(18, 80)  # Random age between 18 and 80\n",
        "    height = random.uniform(140, 200)  # Random height in centimeters (140 cm to 200 cm)\n",
        "    weight = random.uniform(40, 120)  # Random weight in kilograms (40 kg to 120 kg)\n",
        "\n",
        "    # Calculate BMI\n",
        "    bmi = weight / ((height / 100) ** 2)\n",
        "\n",
        "    # Determine BMI class based on BMI value\n",
        "    if bmi < 18.5:\n",
        "        bmi_class = \"Underweight\"\n",
        "    elif 18.5 <= bmi < 24.9:\n",
        "        bmi_class = \"Normal Weight\"\n",
        "    elif 25 <= bmi < 29.9:\n",
        "        bmi_class = \"Overweight\"\n",
        "    else:\n",
        "        bmi_class = \"Obese\"\n",
        "\n",
        "    # Append data to respective lists\n",
        "    age_data.append(age)\n",
        "    height_data.append(height)\n",
        "    weight_data.append(weight)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Age': age_data,\n",
        "    'Height (cm)': height_data,\n",
        "    'Weight (kg)': weight_data,\n",
        "    'BMI': [round(weight / ((height / 100) ** 2), 2) for weight, height in zip(weight_data, height_data)],\n",
        "    'BMI Class': [bmi_class for _ in range(1000)]\n",
        "})\n",
        "\n",
        "# Save the data to a CSV file\n",
        "data.to_csv('bmi.csv', index=False)\n",
        "\n",
        "print(\"Data collection and saving complete.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P5rjlclf9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc1c8cd-27a3-427c-de4d-8edd604af6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 0 articles and saved to 'articles.json'.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# Function to fetch articles from Google Scholar\n",
        "def fetch_google_scholar_articles(query, start_year, end_year, num_articles):\n",
        "    # Base URL for Google Scholar\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    articles = []  # List to store the collected articles\n",
        "\n",
        "    # Loop to paginate through search results (10 results per page)\n",
        "    for start in range(0, num_articles, 10):\n",
        "        # Parameters for the search query, including keywords and date range\n",
        "        params = {\n",
        "            \"q\": query,             # Search query\n",
        "            \"as_ylo\": start_year,   # Start year of publication range\n",
        "            \"as_yhi\": end_year,     # End year of publication range\n",
        "            \"start\": start          # Pagination offset\n",
        "        }\n",
        "\n",
        "        # User-Agent header to mimic a web browser\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.1234.0 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        # Send a GET request to the Google Scholar search URL with parameters and headers\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "        # Check if the response is successful (HTTP status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the response using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all the search result div elements\n",
        "            results = soup.find_all('div', {'class': 'gs_ri'})\n",
        "\n",
        "            # Iterate through each search result\n",
        "            for result in results:\n",
        "                article = {}  # Dictionary to store article information\n",
        "\n",
        "                # Extract the title of the article (inside an h3 element with class 'gs_rt')\n",
        "                title = result.find('h3', {'class': 'gs_rt'})\n",
        "                if title:\n",
        "                    article['title'] = title.text  # Store the title in the dictionary\n",
        "\n",
        "                # Extract the venue/journal/conference information (inside a div element with class 'gs_a')\n",
        "                venue = result.find('div', {'class': 'gs_a'})\n",
        "                if venue:\n",
        "                    article['venue'] = venue.text  # Store the venue information\n",
        "\n",
        "                # Extract the publication year (from the 'gs_a' div)\n",
        "                year = result.find('div', {'class': 'gs_a'})\n",
        "                if year:\n",
        "                   # Split the text and get the last part (usually the year), then strip whitespace\n",
        "                    year = year.text.split('-')[-1].strip()\n",
        "                    article['year'] = year  # Store the year in the dictionary\n",
        "\n",
        "                # Extract the authors (from the 'gs_a' div)\n",
        "                authors = result.find('div', {'class': 'gs_a'})\n",
        "                if authors:\n",
        "                    # Split the text and get the first part (usually the authors), then strip whitespace\n",
        "                    authors = authors.text.split('-')[0].strip()\n",
        "                    article['authors'] = authors  # Store the authors in the dictionary\n",
        "\n",
        "                # Extract the abstract (inside a div element with class 'gs_rs')\n",
        "                abstract = result.find('div', {'class': 'gs_rs'})\n",
        "                if abstract:\n",
        "                    article['abstract'] = abstract.text  # Store the abstract in the dictionary\n",
        "\n",
        "                # Append the article dictionary to the list of articles\n",
        "                articles.append(article)\n",
        "\n",
        "                # Check if the desired number of articles has been collected\n",
        "                if len(articles) >= num_articles:\n",
        "                    return articles\n",
        "\n",
        "    return articles\n",
        "    # Main program\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"information retrieval\"  # Keyword for the search\n",
        "    start_year = 2013                # Start year of publication range\n",
        "    end_year = 2023                  # End year of publication range\n",
        "    num_articles = 1000              # Desired number of articles to collect\n",
        "\n",
        "    # Call the fetch_google_scholar_articles function to collect articles\n",
        "    articles = fetch_google_scholar_articles(keyword, start_year, end_year, num_articles)\n",
        "\n",
        "    # Save the collected articles to a JSON file\n",
        "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(articles, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    # Print the number of collected articles and a confirmation message\n",
        "    print(f\"Collected {len(articles)} articles and saved to 'articles.json'.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do either of the question-4 tasks given below."
      ],
      "metadata": {
        "id": "yCQpbJnwTxAB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FymVNKVi9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b576f7-f30e-4e08-c433-354806746f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw\n",
        "!pip install --upgrade praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUIvQoG9hqDS",
        "outputId": "dcac068c-9833-48ca-bed4-9ef59777bdff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/191.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/191.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.6.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.7.22)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.3.0 update-checker-0.18.0\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.6.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the Instagram profile or hashtag URL to scrape\n",
        "url = \"https://www.instagram.com/explore/tags/your_hashtag/\"\n",
        "\n",
        "# Send a GET request to the Instagram page\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find and extract information for each post on the page\n",
        "posts = soup.find_all('div', class_='v1Nh3')\n",
        "\n",
        "# Loop through the posts and extract the desired information\n",
        "for post in posts:\n",
        "    # Extract post details such as user_name, posted_time, and text\n",
        "    user_name = post.find('a', href=True)['href'].split('/')[1]  # Extract username from href\n",
        "    posted_time = post.find('time')['datetime']\n",
        "    text = post.find('img')['alt']  # Assuming the alt text contains the post caption\n",
        "\n",
        "    print(\"User_name:\", user_name)\n",
        "    print(\"Posted_time:\", posted_time)\n",
        "    print(\"Text:\", text)\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "eHmSeOzFhwHh"
      },
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}